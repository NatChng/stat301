% \documentclass{article}
\documentclass[8pt,landscape,a4paper, fleqn, dvipsnames]{extarticle}
% !TEX enableShellEscape = yes
% (The above line makes atom's latex package compile with -shell-escape
% for minted, and is just ignored by other systems.)
\usepackage{minted}
\usepackage{minted} \BeforeBeginEnvironment{minted}{\begingroup\color{black}} \AfterEndEnvironment{minted}{\endgroup} \setminted{autogobble,breaklines,breakanywhere,linenos}

\usepackage{minted}
\usepackage{graphicx} % Required for inserting images
\usepackage{multicol}
\usepackage{amsmath,amssymb}
% \usepackage[fleqn]{amsmath}
\usepackage{mathtools}
\usepackage{wrapfig}
\usepackage[top=0.5cm,bottom=0.5cm,left=0.6cm,right=0.6cm]{geometry}

% section spacing
\usepackage{titlesec}
\titlespacing*{\section}{0pt}{0.4\baselineskip}{0.2\baselineskip}

% Math
\def\R{\mathbb{R}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\norm}[1]{\lVert #1 \rVert}
\newcommand{\mat}[1]{\begin{bmatrix}#1\end{bmatrix}}

% matrix
\newcommand{\zm}{%
  \begin{bmatrix}
    - z_1^T - \\
    - z_2^T - \\
    \ldots \\
    - z_n^T -
  \end{bmatrix}%
}
\newcommand{\wm}{%
  \begin{bmatrix}
    - w_1^T - \\
    - w_2^T - \\
    \ldots \\
    - w_k^T -
  \end{bmatrix}%
}
\newcommand{\wmm}{%
  \begin{bmatrix}
    \mid &\mid  &\ldots &\mid\\
    w^1 & w^2 &\ldots &w^d\\
    \mid &\mid  &\ldots &\mid
  \end{bmatrix}%
}

\usepackage{enumitem}
\setlist{nolistsep, itemsep = 1pt, leftmargin = 1em}
\setlist[2]{nolistsep, topsep = 1pt, leftmargin = 1em}
\renewcommand{\labelitemii}{$\circ$}
\renewcommand{\labelitemiii}{$\rightarrow$}

\usepackage{xcolor}
\definecolor{mygray}{rgb}{0.8,0.8,0.8}
\usepackage{listings}
\lstset{%
basicstyle=\ttfamily,
breaklines = true,
}
\usepackage{realboxes}
\usepackage{soul}

\begin{document}
\begin{multicols*}{4}

\section*{\ul{Useful Coding Pipelines (blank w/ annotation)}}
\begin{itemize}
\item Linear Regression
\begin{lstlisting}
    lm(response ~ predictor +/* predictor, data = ...) 
    #can specify 0 as reference level to make estimates the mean instead of difference from reference level
\end{lstlisting}
\item Logistic Regression
\begin{lstlisting}
    glm(response ~ predictor, data = ..., family = binomial)
\end{lstlisting}
\item Poisson Regression
\begin{lstlisting}
    glm(response ~ predictor, data = ..., family = "poisson")
\end{lstlisting}
\item Visualizations
\begin{itemize}
    \item Scatterplot with Estimated Regression
    \begin{lstlisting}
    ... <- 
        data %>%
        ggplot(aes(x = ..., y = ...)) +
        geom_point() +
        geom_smooth(method = lm/"glm", se = FALSE, size = 1.5)
    \end{lstlisting}
    \item Adding upper and lower limits to histogram (0.05 significance in this example)
    \begin{lstlisting}
        ... +
        geom_vline(xintercept = quantile(lm_boot$boot_slope, 0.025), col = 'blue', size = 1) +
    geom_vline(xintercept = quantile(lm_boot$boot_slope, 0.975), col = 'blue', size = 1) 
    \end{lstlisting}
    \end{itemize}
    \item Stuff
    \begin{itemize}
        \item Tidy w/ confidence interval
        \begin{lstlisting}
    ... <- tidy(lm_here, conf.int = FALSE/TRUE, conf.level = ...) 
    #tidy model to get p-value, confidence intervals. Default is 0.95 and conf.int = FALSE
        \end{lstlisting}
        \item Tidy to get odds
        \begin{lstlisting}
            tidy(logistic_model_here, exponentiate = TRUE)
        \end{lstlisting}
    \end{itemize}
    \item \begin{itemize}
        \item Bootstrapping w/ Infer
        \begin{lstlisting}
            bootstrap_slope_infer <- 
#   ... %>% 
#   specify(formula = ... ~ ...) %>%
#   generate(reps = ..., type = "bootstrap") %>% 
#   calculate(stat = "slope")
        \end{lstlisting}
        \item Regression Estimates (Fitting Regression to Bootstrap Samples)
        \begin{lstlisting}
    lm_boot250 <- replicate(sets, {
  sample_n(data, size = ..., replace = TRUE) %>%
    lm(formula = ... ~ ..., data = .) %>%
    .$coef
}) 
        \end{lstlisting}
        \begin{lstlisting}
    bootstrap_slope_infer <- 
    US_cancer_sample250 %>% 
    specify(formula = TARGET_deathRate ~ povertyPercent) %>%
    generate(reps = 1000, type = "bootstrap") %>% 
    calculate(stat = "slope")
        \end{lstlisting}
        \item Prediction (Log R using Predict)
        \begin{lstlisting}
            predict(modelname, tibble(predictor1 = ..., predictor2 = ..., cont.), type = ...)
            # type = "link" for odds, "response" for classification
        \end{lstlisting}
        \item ANOVA (comparing nested models)
        \begin{lstlisting}
            anova(model 1, model 2)
            anova(model 1, lm(response ~ 1, data = ...)) # null model comparison
        \end{lstlisting}
        \item Split to training and testing pipeline
        \begin{lstlisting}
            fat_sample <- 
    fat_sample %>%
    mutate(id = row_number())

training_fat <- 
    fat_sample %>%
    slice_sample(prop = 0.70, replace = FALSE)

selection_set_fat <- 
    fat_sample %>%
    anti_join(training_fat, by = "id")

training_fat <- 
    training_fat %>% 
    select(-"id")

selection_set_fat <- 
    selection_set_fat %>% 
    select(-"id")
        \end{lstlisting}
        \item Forward/backward selection:
        \begin{lstlisting}
            regsubsets(
        x = response ~ ., 
        nvmax = num_predictors,
        data = ...,
        method = "backward"/forward,
    )
        \end{lstlisting}
        \item CI for Prediction:
        \begin{lstlisting}
            properties_sample  %>% 
    select(assess_val, BLDG_METRE) %>% 
    cbind(predict(lm_sample, interval="confidence", se.fit=TRUE)$fit)
        \end{lstlisting}
        \item Prediction Interval:
        \begin{lstlisting}
        properties_sample  %>% 
    select(assess_val, BLDG_METRE) %>% 
    cbind(predict(lm_sample, interval="prediction"))    
        \end{lstlisting}
        \item Training and Testing Split:
        \begin{lstlisting}
    training_fat <- 
    fat_sample %>%
    slice_sample(prop = 0.7, replace = FALSE)

selection_set_fat <- 
    fat_sample %>%
    anti_join(training_fat, by = "id")

training_fat <- 
    training_fat %>% 
    select(-"id")

selection_set_fat <- 
    selection_set_fat %>% 
    select(-"id")
        \end{lstlisting}
    \end{itemize}
    \item Forward/Backward Selection with regsubsets():
    \begin{lstlisting}
fat_backward_sel <- regsubsets(
  x=brozek ~ ., 
  nvmax=14,
  data=selection_set_fat,
  method="backward/foward",
)

fat_backward_sel

fat_bwd_summary <- summary(fat_backward_sel)
    \end{lstlisting}
    \item CI for prediction:
    \begin{lstlisting}
properties_cip_90 <- 
    properties_sample %>% 
    select(assess_val, BLDG_METRE) %>% 
    cbind(predict(lm_sample, interval="confidence", level = 0.90, se.fit=TRUE)$fit)
    \end{lstlisting}
    \item Prediction Interval:
    \begin{lstlisting}
properties_pi <- 
    properties_sample  %>% 
    select(assess_val, BLDG_METRE) %>% 
    cbind(predict(lm_sample, interval="prediction", level = 0.95 default)
    \end{lstlisting}
    \item Finding $R^2$ and MSE using metrics:
    \begin{lstlisting}
housing_test_metrics <- 
    testing_housing %>%
    metrics(truth = SalePrice, estimate = pred_full_OLS)

housing_test_metrics
    \end{lstlisting}
    \item LASSO using glmnet():
    \begin{lstlisting}
lasso_cancer_fitted <- # The variable to store the fitted model
    glmnet(
        x = cancer_train |> select(-lpsa),  # First argument is the covariates (note that we need at least two columns)
        y = cancer_train$lpsa,  # Second argument is the response
        alpha = 1, # This is the default and is equivalent to LASSO 
        nlambda = 100, # the number of lambda values to try (default is 100)
        #lambda  = ..., # alternatively, the values of lambda you want try
        standardize = TRUE, # Default value is TRUE
    )
    \end{lstlisting}
    \item Prediction using LASSO:
    \begin{lstlisting}
predict(lasso_cancer_fitted, # Fitted model
    newx = cancer_train |> select(-svi) |> as.matrix(), # A matrix (yes, tibble don't work) containing the data for prediction,
    s = c(0.4, 0.1) # one or more values of lambda
   ) 
    \end{lstlisting}
    \item Cross Validation using cv.glm():
    \begin{lstlisting}
cv_logistic <- 
    cv.glm(
        glmfit = breast_cancer_logistic_model, 
        data = breast_cancer_train, 
        K = 10, cost = misclassification_rate )
    \end{lstlisting}
    \item Confusion Matrix:
    \begin{lstlisting}
breast_cancer_confusion_matrix <- 
    confusionMatrix(
    data = as.factor(breast_cancer_pred_class),
    reference = as.factor(breast_cancer_train$target),
    positive = '1'
)
    \end{lstlisting}
    \item Pairwise frequentist hypothesis testing:
    \begin{lstlisting}
pairwise_comparisons <- pairwise.prop.test(x = successes,
  n = trials,
  p.adjust.method = "bonferroni/pocock/OF", 
  alternative = "two.sided/greater/less",
)
    \end{lstlisting}
    \item Getting Pocock critical value:
    \begin{lstlisting}
design_pocock <- gsDesign(k = 20, #number of interim analysis planned
    test.type = 1, # for one-sided tests
    delta = 0, # default effect size
    alpha = 0.05, #type I error rate
    beta = 0.2, # type II error rate
    sfu = 'Pocock')
                          
crit_pocock <- design_pocock$upper$bound
    \end{lstlisting}
    \item Getting O'Brien Flemming Critical Values:
    \begin{lstlisting}
design_of <- gsDesign(k = 10, #number of interim analysis planned
    test.type = 1, # for one-sided tests
    delta = 0, # default effect size
    alpha = 0.05, #type I error rate
    beta = 0.2, # type II error rate
    sfu = 'OF')

crit_of_10 <- design_of$upper$bound
    \end{lstlisting}
\end{itemize}

\section*{\ul{Simple Linear Regression and Multiple Linear Regression}}
\begin{itemize}
    \item 3 aspects of modelling:
    \begin{itemize}
        \item Estimation: Estimate the true unknown relation between response and input variables
        \item Inference: Use model to infer information about the unknown relation between variables
        \item Prediction: Use model to predict the value of the response for new observations
    \end{itemize}
    \item General formula for SLR: $E[Y|X]= \beta_1 X + \beta_0 + \epsilon$
    \begin{itemize}
        \item "For an increase of 1 in input, we'd \textbf{expect to observe} an increase of $\beta_1 = ...$ of response \textbf{with all other variables constant}". DO NOT say effect/caused (correlation, not causation)
        \item Error $\epsilon$: All external factors affecting Y other than X. Assume iid and $E[\epsilon|X] = 0$
        \item When determining the best line, choose the line that minimizes the sum of the vertical distance of points to the line (minimize distance of output variable, rather than euclidean distance)
        \item Slope estimator $\hat{\beta}_1 = \frac{\sum^n_{i = 1}(X_i - \Bar{X})(Y_i - \Bar{Y})}{\sum^n_{i = 1}(X_i - \Bar{X})^2}$
    \end{itemize}
    \item Additive MLR: Contains categorical and numerical variables, yields two lines with the same slope and different intercept
    \begin{itemize}
        \item Assume that change in the response per unit change of predictor DOES NOT DEPEND on other predictors
        \item Default level is selected by alpha order if not specified. Number of dummy variables is n - 1 (1 less than number of categories)
    \end{itemize}
    \item Interactive (multiplicative) MLR: Contains categorical and numerical variables, includes product between input variables (covariates) and results in two lines with different slopes and intercepts.
    \item Using the deathRate ~ povertyPercent + state example:
    \begin{itemize}
        \item We'd get something like $Y_i = \beta_0 + \beta_1 * 1 + \beta_2 *$ povertyPercent + $\beta_3 * 1 * $ povertyPercent + $\epsilon_i$
        \begin{itemize}
            \item $\beta_0$ is the intercept of our reference line (Indiana)
            \item $\beta_1$ is the coefficient of the dummy variable, ie. difference between intercepts of both lines (the 1 means it's an interaction term)
            \item $\beta_2$ is the slope of the reference line
            \item $\beta_3$ is the difference between slopes of both lines 
        \end{itemize}
        \item In the text, we used Indiana as our reference for mean mortality, therefore stateKansas = -20.286 means the mean mortality is 20.286 less than Indiana
    \end{itemize}
    \item A (UNIT NAME) increase in X is \textbf{associated} with an \textbf{expected} change in Y of $\beta_1$ (unit name), keeping all other variables constant
    \begin{lstlisting}
        
    \end{lstlisting}
\end{itemize}

\section*{\ul{Assumption Violations, Logistic Regression, Poisson Regression}}
\begin{itemize}
    \item Reverse causality example: "When parents helped with schoolwork, kids usually performed worse". It may be possible kids not performing well in school received regular parental help.
    \item Simpson's paradox: Sign of the correlation flip when comparing the entire population vs specific strata
    \item Confounding variable: Variable that causes change in both the response and at least one input variable
    \item Establishing causal effects is challenging. It depends on how data were collected and the statistical methods used to analyze the data
    \item Experimental design: The manner in the randomization of experimental units to treatments is carried out and how data are collected
    \begin{itemize}
        \item Completely random design (CRD): Experimental units are randomized throughout the data layout - no correlation between observations
        \item In CRD, observed and observed confounders are balanced on average
        \item Randomized block design (RBD): Splits experimental units into homogeneous blocks to remove variation from nuisance factors, then assigns treatments to each block. Blocks are similar in all aspects except for treatment (ex. subjects of similar age groups are blocks)
        \item In RBD, only observed confounders are balanced so only average treatment effects can be estimated
    \end{itemize}
    \item Observational data: Collecting data by measuring variables/surveying without applying any treatment. Causal effects cannot be established as treatments are not controlled
    \begin{itemize}
        \item Omitting a confounding factor affects estimates of the regression - including it in the regression solves accuracy issues but may be difficult to do
        \item You can stratify to check association within subgroups that share common values of the confounder
    \end{itemize}
    \item Assumptions of linear regression models (LINEM)
    \begin{itemize}
        \item L: \textbf{Linear} relation
        \begin{itemize}
            \item Dubious model if not followed
            \item Can remedy by adding transformed covariates (ex. $X^2, \sqrt{X}. log(X)$)
        \end{itemize} 
        \item I: Errors are \textbf{independent} 
        \begin{itemize}
            \item Inflates SE (causes errors in CI and hypothesis testing)
            \item Remedy is to adapt model (ex. time series analysis)
        \end{itemize}
        \item N: Conditional distribution of the error terms is \textbf{Normal} (if errors are iid, thus conditional distribution of response is linear). 
        \begin{itemize}
            \item Use QQ plot or histogram of residuals
            \item Affects sampling dist, but can use CLT provided n is large or bootstrap
        \end{itemize}
        \item E: \textbf{Equal variance} of error terms (we assume homoscedasticity, but in reality, a data tend to be heteroscedastic) -> 
        \begin{itemize}
            \item Affects SE (and therefore, CI and hypothesis test results)
            \item You can transform response to stabilize variance (ex. $\sqrt{Y}, log(Y)$) but it only works for positive data. Can also bootstrap for CIs and hypothesis tests
        \end{itemize}
        \item M: \textbf{Multicollinearity}
    \end{itemize}
    \item Multicollinearity occurs when multiple input variables are correlated - info from one var is masked by another
    \begin{itemize}
        \item Collinearity is between 2 predictors
        \item LS estimator is inflated, very variable, and unreliable
        \item SE of LS estimators are large
        \item VIF checks increase in SE of coefficients fitted alone vs other variables. If $\geq$ 5 or 10, suggests multicollinearity
        \item You know if removing the variable with the largest VIF worked if the VIFs of other variables decreased
    \end{itemize}
    \item The bootstrap sampling dist of the slope estimator is tighter as the size of the sample bootstrapped from increases
    \item Poisson is $log(E[Y|X]) = \beta_0 + \beta_1*X_1 + ... + \beta_p * X_p$
    \item Logistic is binary data (ie. y = 1 or 0, $E[Y_i] = P(y_i = 1)$) and predicts the probability of a binary event occurring  
    \begin{itemize}
        \item Logit (log odds): $log(\frac{P(y_i = 1 | X_i)}{1 - P(y_i = 1 | X_i)}) = \beta_0 + \beta_1*X_1 + ... + \beta_p * X_p$. As probability increases, odds increases
        \item Odds formula: $\frac{P(y_i = 1 | X_i)}{1 - P(y_i = 1 | X_i)} = e^{\beta_0 + \beta_1*X_1 + ... + \beta_p * X_p}$
        \item Alternatively: $P(y_i = 1 | X_i) = \frac{e^{\beta_0 + \beta_1 * X_{i1}}}{1 + e^{\beta_0 + \beta_1 * X_{i1}}}$
        \item Here's an example of how to interpret logit:
        \begin{itemize}
            \item Intercept: $\beta_0 = -0.9$ If the X was 0 (unit), the logit is expected to be -0.9
            \item Slope: $\beta_1 = 0.01$ An increase of 1 (unit) is associated with an increase in the logit of 0.01
        \end{itemize}
        \item Use tidy to find odds or mutate and calculate exponential of estimate exp(estimate)
        \begin{itemize}
            \item Intercept: $e^{\beta_0} = 0.4$ if the X was 0 (unit), the odds is expected to be 0.4
            \item Slope: $e^{\beta_1} = 1.1$ it multiplies the odds by 1.1 (ie. odds increases in 10 percent of its value). So if we had sexmale, male passengers odds were 1.1 times higher than female passengers
        \end{itemize}
    \end{itemize}
    \item Here's an example of an interactive Logistic Regression:
    \begin{itemize}
        \item Logit($p_{survival}$) = $\beta_0 + \beta_2 *$ fare, if $X_{male} = 0$.  Logit($p_{survival}$) = $\beta_0 + \beta_1 + (\beta_2 + \beta_3) *$ fare, if $X_{male} = 1$
        \item Odds $\frac{p_{survival}}{1 - p_{survival}} = e^{\beta_0} e^{\beta_2 * fare}$ if $X_{male} = 0$ or $e^{\beta_0 + \beta_1} e^{(\beta_2 + \beta_3)* fare}$ if $X_{male} = 1$
    \end{itemize}
    \item Poisson regression models count data, ie. response variable is number of times an event occurs in a fixed time interval (non-negative)
    \begin{itemize}
        \item $Y_i | X_i ~ Poisson(\lambda_i)$, $E[Y_i | X_i ] = \lambda_i$, $Var[Y_i | X_i ] = \lambda_i$
        \item Models log of expected count, ie. $log(\lambda_i) = \beta_0 + \beta_1 * X_i$
        \item An increase in 1 unit X is associated with an expected change in the average number of Y by a factor of $e^{0.0014} = 1.00144$ ie. an increase of 0.14 percent
    \end{itemize}
\end{itemize}

\section*{\ul{Model Assessment and Selection, Predictive Modelling}}
\begin{itemize}
    \item Absolute measures of fit:
    \begin{itemize}
        \item RSS = residuals = $\Sigma_{i = 1}^n (y_i - \hat{y_i})^2$ (sum of difference between finding and prediction squared, ie. difference between actual and prediction squared, aka distance from point to the regression line squared)
        \item RSE = residual standard error = $\sqrt{\frac{1}{n - p - 1} RSS}$ p is number of covariates
        \item Mean squared error = RSS/n
    \end{itemize}
    \item Relative measures of fit:
    \begin{itemize}
        \item ESS = explained sum squares = $\Sigma_{i = 1}^n (\hat{y_i} - \bar{y})^2$. Y-hat predicts y using the LR, y-bar predicts y without a model. If the model is better than nothing, this should be large
        \item TSS = total sum squared = $\Sigma_{i = 1}^n (y_i - \bar{y})^2$ (difference of residuals from the null), if scaled correctly it's the sample variance of Y (ie. difference of each point from the mean squared)
        \item TSS = RSS + ESS
    \end{itemize}
    \item $R^2$ is ESS/TSS or 1 - RSS/TSS. 
    \begin{itemize}
        \item Proportion of variance (TSS) explained by the model (ESS).
        \item It measures in-sample and not test sets (out-of-sample)
        \item More input variables = larger regardless of relevance
    \end{itemize}
    \item Adjusted $R^2$ = 1 - $\frac{RSS/(n - p - 1)}{TSS/(n - 1)}$
    \item Use glance(model) to compute $R^2$, adjusted $R^2$, statistic, df, and p value
    \item glance() equivalencies:
    \begin{lstlisting}
anova(lm(response ~ 1, data), lm(response ~ explanatory), data))

tidy(lm(response ~ explanatory, data)) %>% filter(term == 'explanatory') %>% pull(p.value)
    \end{lstlisting}
    \item Nested models are 2 models in which one contains the other model + more coefficients. We test these using the F-statistic (anova)
    \item Forward selection:
    \begin{itemize}
        \item Start with intercept only model
        \item Select best model of each size and keep adding variables until you get to full model
        \item Pick the best one (higher adj r2). We will get an optimal size, pick those with a star in the row of the optimal size
    \end{itemize}
    \item Backward select is the same, but we start from the full model and remove 1 variable each time
    \item MSE and RMSE:
    \begin{itemize}
        \item MSE = mean(prediction - response)$^2$ (Y units squared)
        \item RMSE is the sqrt of MSE (same units as Y)
    \end{itemize}
    \item BIC penalizes complex models more than AIC. Best model is the one with the lowest AIC/BIC
    \item CI for prediction: Interval for the true mean response value for given levels of explanatory variables (CI of the line)
    \begin{itemize}
        \item with 90$\%$ confidence, the expected value of a house of size 97 mts is between 301274 and 363407 (rounded)
        \item Centred at Y-hat. At lower significance level, it's target (ie. 95$\%$ CIP is wider than 90)
        \item Narrower when values of covariates are closer to their mean, and wider as covariates move away from the mean
    \end{itemize}
    \item Prediction interval: Interval the actual response value for a NEW OBSERVATION at the given levels of the covariates (CI of the point).
    \begin{itemize}
        \item With 95$\%$ confidence, the brozek for a man that weighs 154.25lbs will be between 2.824 and 27.072
    \end{itemize}
    \item Precision: true positive / (true pos + false pos)
    \item Accuracy: (true pos + true neg) / n
    \item When interpreting an ANOVA: There is sufficient/insufficient evidence to reject the null hypothesis that model 1 is equivalent to model 2.
    \item LASSO: Linear regression method iwth regularization (adds penalty term to LoF of the regression)
    \begin{itemize}
        \item Prevents overfitting and reduces model's variance
        \item Penalty is the sum of the absolute values of the coefficients (sets some of them to zero when this value is sufficiently large), it can also deal with multicollinearity
        \item May not perform well when some covariates are important but have small coefficients
    \end{itemize}
\end{itemize}

\section*{\ul{Predictive Modelling}}
\begin{itemize}
    \item LASSO:
    \begin{itemize}
        \item You must standardize covariates, as LASSO penalizes large coefficients, as such, we must standardize the units of covariates (ie. subtract the mean, then divide result by sd)
        \item Fit model using OLS
        \item When you increase the penalty $\lambda$, the absolute value of each coefficient is reduced. This also improves the RSS
        \item When we fit categorical data, we use dummy variables (OHE) this doesn't quite make sense
    \end{itemize}
    \item LASSO Inference:
    \begin{itemize}
        \item LASSO gives biased estimates and CIs aren't immediately available
        \item You can deal with this using data split. In the first split, run LASSO to select variables. In the second split, fit an OLD and use the inference
        \item Splitting loses part of the datset (this increases standard error as sample size decreases)
    \end{itemize}
    \item You can build a regularized regression using glmnet() and cv.glmnet() (remember, alpha = 0 is ridge, = 1 is LASSO):
    \begin{lstlisting}
# step 1: using matrices
Housing_X_train <- as.matrix(training_Housing[,-20])
Housing_Y_train <- as.matrix(training_Housing[,20])

Housing_X_test <- as.matrix(testing_Housing[,-20])
Housing_Y_test <- as.matrix(testing_Housing[,20])

# step 2 and 3: fitting LASSO on a given grid (try using the default as well)
Housing_LASSO <- glmnet(
  x = Housing_X_train, y = Housing_Y_train,
  alpha = 1,
  lambda = exp(seq(5, 12, 0.1))
)

#step 4: extracting estimated coefficients for a given level of regularization
coef(Housing_LASSO, s = 40000 or Housing_LASSO$lambda.min)

# step 5: selecting best lambda by CV
Housing_cv_LASSO <- cv.glmnet(
  x = Housing_X_train, y = Housing_Y_train,
  alpha = 1,
  lambda = exp(seq(5, 12, 0.1))
)

# Full LS predictions
Housing_test_pred_full_OLS <- predict(Housing_full_OLS, newdata = testing_Housing)

# LASSO predictions with lambda.min
Housing_test_pred_LASSO_min <- predict(Housing_cv_LASSO, 
    newx = Housing_X_test, 
    s = "lambda.min")
    \end{lstlisting}
    \item Lambda.min provides max AUC, lambda.1se is highest lambda for which the model has an AUC within one SE of the max
    \item If we split the data and use different parts for model selection and type 1 error, the F-test after the forward selection is closer to the significance level
    \item The sampling distribution of the lasso estimator is not centered around the true $\beta_1$
    \item LASSO has 2 main problems:
    \begin{itemize}
        \item Biased estimators: This can be addressed by fitting least squares on the variables selected by LASSO (post-lasso)
        \item Post-inference: Fitting an LS regression after LASSO, we use the data to select the variables and to conduct inference. We can't rely on the linear model, unless we split the data
    \end{itemize}
    \item LASSO can fit an lm on datasets with high multicollinearity (remember, vif >= 5 or 10)
    \item Classification error:
    \begin{itemize}
        \item Training error is usually less than cv error (unless by some fluke)
    \end{itemize}
    \item Confusion Matrix provides case counts (success-fail horizontal, success-fail vertical)
    \begin{itemize}
        \item Sensitivity is number of correct success predictions divided by true successes (true success / true success + false neg) (ideally closer to 1)
        \item Specificity is number of correct failure predictions divided by true failures (true neg / true neg + false pos) (ideally closer to 1)
        \item Cohen's Kappa: How often predictions and classification coincide by chance. Range from -1 to 1 (ideally, close to 1)
        \item If you decrease threshold, specificity decreases and sensitivity increases
    \end{itemize}
    \item Better ROC arches towards the top left corner (ie. it's better than a random classifier) (1). Bad ROC arches towards the lower right (0).
\end{itemize}

\section*{\ul{A/B Testing}}
\begin{itemize}
    \item Sequential testing: Data are collected and analyzed in multiple stages, test can be stopped early if a significant difference is observed
    \begin{itemize}
        \item Increases probability of Type 1 error (multiple tests are being performed)
        \item Addressed using group sequential methods. 
    \end{itemize}
    \item Group sequential methods:
    \begin{itemize}
        \item Bonferroni divides significance level by k (number of tests performed) to have a standard p value across tests. This is really conservative an increases chance of type 2 error. You can also just multiply the p-values you receive, or adjust the critical value
        \item Pocock sets a symmetric critical value requiring constant significance level for each group. 
        \begin{itemize}
            \item This will be lower than the Bonferroni cv (less conservative) but higher than the unadjusted (the p value is higher than BF, lower than unadjusted)
            \item As number of peeks increases, crit values increase (type 1 error increases, thus need more conservative CV)
        \end{itemize}
        \item O'Brien-Fleming is asymmetric boundary with high significance level for the first groups, then gradually decreases for subsequent groups.
    \end{itemize}
    \item Power analysis estimates the min sample size required given a desired significance, expected difference in means (or just test stat) and power
\end{itemize}

\end{multicols*}
\end{document}